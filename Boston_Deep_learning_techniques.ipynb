{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "85crw6GJKTME"
   },
   "source": [
    "# Boston House price prediction \n",
    "\n",
    "The goal of this notebook is to start with basic Deep Learning and an introduction to PyTorch. It will build a simple neural network to solve a regression problem using the Boston Housing dataset and also a classifier for the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TQXxIs2xKLRN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1208,
     "status": "ok",
     "timestamp": 1598995388293,
     "user": {
      "displayName": "Falconi Nicasio",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5bfS33g4yrFOAB7gQSeDbZZUajU5WE1gMNczk7Q=s64",
      "userId": "15057034274223364124"
     },
     "user_tz": 420
    },
    "id": "r-832npfp7Co",
    "outputId": "c5c4b925-ed86-45d4-bb19-c562ec013ade"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1607370156314/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c6D6YzR-SipZ"
   },
   "source": [
    "## Regression - Boston Housing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XJZZyjMHV1o7"
   },
   "source": [
    "### Import Boston Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GcWRFh3MLiYy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "boston = load_boston()\n",
    "\n",
    "X = boston['data']\n",
    "y = boston['target']\n",
    "\n",
    "in_features = X.shape[1]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ilhmmFIvWAVC"
   },
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LEZnHSfiXtTc"
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOODLE\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)\n",
    "\n",
    "# load data\n",
    "boston = load_boston()\n",
    "\n",
    "X = boston['data']\n",
    "y = boston['target']\n",
    "\n",
    "in_features = X.shape[1]\n",
    "X.shape\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "## Scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test) #Transform test set with the same constants\n",
    "\n",
    "# convert numpy arrays to tensors\n",
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "\n",
    "# create TensorDataset in PyTorch\n",
    "boston_train = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "#boston_test = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "# use the sequential API makes things simple\n",
    "model = torch.nn.Sequential(    \n",
    "    torch.nn.Linear(D_in, D_hidden),   # X.matmul(W1)\n",
    "    nn.ReLU(), #nn.Sigmoid()           # Relu( X.matmul(W1))\n",
    "    nn.Linear(in_features=D_hidden, out_features=D_out)       # Relu( X.matmul(W1)).matmul(W2)\n",
    "    \n",
    ")\n",
    "# loss scaffolding layer\n",
    "loss_fn = torch.nn.MSELoss(size_average=True)\n",
    "\n",
    "criterion = nn.MSELoss(size_average=False)\n",
    "#criterion = nn.CrossEntropyLoss()  #for classfication \n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also do normalization to the range of $(0; 1)$ to make our data insensitive to the scale of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T22:33:49.926210Z",
     "start_time": "2018-11-28T22:33:49.921539Z"
    }
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we're going to learn normalization constants only on training set. That's done because the assumption is that test set is unreachable during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T22:33:50.649656Z",
     "start_time": "2018-11-28T22:33:50.641127Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform test set with the same constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-28T22:33:52.257273Z",
     "start_time": "2018-11-28T22:33:52.253165Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ACbxJTYVWH7B"
   },
   "outputs": [],
   "source": [
    "# convert numpy arrays to tensors\n",
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "y_test_tensor = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LEND1wZBSrj9"
   },
   "outputs": [],
   "source": [
    "# create TensorDataset\n",
    "boston_train = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "#boston_test = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wGoocKUfSr-5"
   },
   "outputs": [],
   "source": [
    "# create TensorDataset\n",
    "boston_train = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "#boston_test = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "# create dataloader\n",
    "batch_size = 96\n",
    "trainloader_boston = torch.utils.data.DataLoader(boston_train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "#testloader_boston = torch.utils.data.DataLoader(boston_test, batch_size=X_test.shape[0], shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zNdIUlo_SsIs"
   },
   "source": [
    "### RegNet: Construct simple fully connected neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dH_wLQeXSsCb"
   },
   "outputs": [],
   "source": [
    "class RegNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_feature, size_hidden, n_output=1):\n",
    "        super(RegNet, self).__init__()\n",
    "        self.predict = torch.nn.Linear(n_feature, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x\n",
    "\n",
    "# do a linear version\n",
    "# add weight_decay l1 regularization\n",
    "# dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dH_wLQeXSsCb"
   },
   "outputs": [],
   "source": [
    "class RegNet_NN(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_feature, size_hidden=10, n_output=1):\n",
    "        super(RegNet, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(in_features, size_hidden)   # hidden layer\n",
    "        self.predict = torch.nn.Linear(size_hidden, n_output)   # output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden(x))      # activation function for hidden layer\n",
    "        x = self.predict(x)             # linear output\n",
    "        return x\n",
    "\n",
    "# do a linear version\n",
    "# add weight_decay l1 regularization\n",
    "# dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4591,
     "status": "ok",
     "timestamp": 1598995391710,
     "user": {
      "displayName": "Falconi Nicasio",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5bfS33g4yrFOAB7gQSeDbZZUajU5WE1gMNczk7Q=s64",
      "userId": "15057034274223364124"
     },
     "user_tz": 420
    },
    "id": "8oXYIdCweGuK",
    "outputId": "7c823981-fea6-4bef-b892-ced6484f23ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegNet(\n",
       "  (predict): Linear(in_features=13, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regnet = RegNet(in_features, 1)\n",
    "\n",
    "# transfer to GPU for GPU Training\n",
    "regnet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oVUiIYALSsL0"
   },
   "source": [
    "### Define Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4587,
     "status": "ok",
     "timestamp": 1598995391712,
     "user": {
      "displayName": "Falconi Nicasio",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5bfS33g4yrFOAB7gQSeDbZZUajU5WE1gMNczk7Q=s64",
      "userId": "15057034274223364124"
     },
     "user_tz": 420
    },
    "id": "_sKe5A_USsOs",
    "outputId": "f6ecc500-3ce4-4fed-cc55-08ccd3209c75"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss(size_average=False)\n",
    "#criterion = nn.CrossEntropyLoss()  #for classfication \n",
    "\n",
    "optimizer = optim.SGD(regnet.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qa8MsM8_euj4"
   },
   "source": [
    "### Train RegNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5059,
     "status": "ok",
     "timestamp": 1598995392192,
     "user": {
      "displayName": "Falconi Nicasio",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5bfS33g4yrFOAB7gQSeDbZZUajU5WE1gMNczk7Q=s64",
      "userId": "15057034274223364124"
     },
     "user_tz": 420
    },
    "id": "M05Psyjye8s7",
    "outputId": "e9c226bc-24cc-495e-e0f7-5a7b5fd99d46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, mini batch loss 1, MSE loss: 652.864\n",
      "Epoch 1, mini batch loss 2, MSE loss: 562.434\n",
      "Epoch 1, mini batch loss 3, MSE loss: 453.523\n",
      "Epoch 1, mini batch loss 4, MSE loss: 237.344\n",
      "Epoch 2, mini batch loss 1, MSE loss: 365.98\n",
      "Epoch 2, mini batch loss 2, MSE loss: 357.844\n",
      "Epoch 2, mini batch loss 3, MSE loss: 365.383\n",
      "Epoch 2, mini batch loss 4, MSE loss: 203.831\n",
      "Epoch 3, mini batch loss 1, MSE loss: 280.562\n",
      "Epoch 3, mini batch loss 2, MSE loss: 264.838\n",
      "Epoch 3, mini batch loss 3, MSE loss: 214.097\n",
      "Epoch 3, mini batch loss 4, MSE loss: 173.203\n",
      "Epoch 4, mini batch loss 1, MSE loss: 247.882\n",
      "Epoch 4, mini batch loss 2, MSE loss: 176.102\n",
      "Epoch 4, mini batch loss 3, MSE loss: 161.133\n",
      "Epoch 4, mini batch loss 4, MSE loss: 134.984\n",
      "Epoch 5, mini batch loss 1, MSE loss: 201.124\n",
      "Epoch 5, mini batch loss 2, MSE loss: 185.657\n",
      "Epoch 5, mini batch loss 3, MSE loss: 127.646\n",
      "Epoch 5, mini batch loss 4, MSE loss: 78.595\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "epochs = range(5)\n",
    "\n",
    "for epoch in epochs:\n",
    "  running_loss = 0.0\n",
    "  for batch, data in enumerate(trainloader_boston):\n",
    "    input, target = data[0].to(device), data[1].to(device)\n",
    "    \n",
    "\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # do forward pass\n",
    "    output = regnet(input.float())\n",
    "\n",
    "    # compute loss and gradients\n",
    "    loss = criterion(output, torch.unsqueeze(target.float(), dim=1))\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # perform gradient update\n",
    "    optimizer.step()\n",
    "\n",
    "    # print statistics\n",
    "    running_loss = loss.item()/batch_size\n",
    "    print(f\"Epoch {epoch+1}, mini batch loss {batch+1}, MSE loss: {np.round(running_loss, 3)}\")\n",
    " \n",
    "print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r4ozSEPg64rJ"
   },
   "outputs": [],
   "source": [
    "# save trained model\n",
    "\n",
    "path = './boston.pth'\n",
    "torch.save(regnet.state_dict(), path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tJIaH5Ct7LEt"
   },
   "source": [
    "### Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5212,
     "status": "ok",
     "timestamp": 1598995392358,
     "user": {
      "displayName": "Falconi Nicasio",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5bfS33g4yrFOAB7gQSeDbZZUajU5WE1gMNczk7Q=s64",
      "userId": "15057034274223364124"
     },
     "user_tz": 420
    },
    "id": "6tlSWn_67Qn7",
    "outputId": "b33f59ca-18d8-4cd9-8625-278e6466a855"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regnet = RegNet(in_features, 10)\n",
    "regnet.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ic84YKHsM-73"
   },
   "outputs": [],
   "source": [
    "# predict test\n",
    "output = regnet(X_test_tensor.float())\n",
    "\n",
    "# calculate loss\n",
    "loss = criterion(output, torch.unsqueeze(y_test_tensor.float(), dim=1)).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t3MHKFl-NYl2"
   },
   "outputs": [],
   "source": [
    "y_pred = output.detach().numpy().reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5197,
     "status": "ok",
     "timestamp": 1598995392360,
     "user": {
      "displayName": "Falconi Nicasio",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5bfS33g4yrFOAB7gQSeDbZZUajU5WE1gMNczk7Q=s64",
      "userId": "15057034274223364124"
     },
     "user_tz": 420
    },
    "id": "IkhkIRxW-8FY",
    "outputId": "47b789d3-7a73-4af2-8140-51fb3925e1de"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23.6</td>\n",
       "      <td>15.446013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.4</td>\n",
       "      <td>15.117799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.6</td>\n",
       "      <td>19.231237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22.8</td>\n",
       "      <td>13.754997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.1</td>\n",
       "      <td>19.793026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>17.1</td>\n",
       "      <td>20.123314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>14.5</td>\n",
       "      <td>17.543381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>50.0</td>\n",
       "      <td>16.857513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>14.3</td>\n",
       "      <td>16.607327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>12.6</td>\n",
       "      <td>19.993469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     price       pred\n",
       "0     23.6  15.446013\n",
       "1     32.4  15.117799\n",
       "2     13.6  19.231237\n",
       "3     22.8  13.754997\n",
       "4     16.1  19.793026\n",
       "..     ...        ...\n",
       "147   17.1  20.123314\n",
       "148   14.5  17.543381\n",
       "149   50.0  16.857513\n",
       "150   14.3  16.607327\n",
       "151   12.6  19.993469\n",
       "\n",
       "[152 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(zip(y_test, y_pred), columns=['price', 'pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5190,
     "status": "ok",
     "timestamp": 1598995392360,
     "user": {
      "displayName": "Falconi Nicasio",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh5bfS33g4yrFOAB7gQSeDbZZUajU5WE1gMNczk7Q=s64",
      "userId": "15057034274223364124"
     },
     "user_tz": 420
    },
    "id": "OzImCkEL-i3n",
    "outputId": "eb9c296a-643d-40f8-9e25-909ba5adef5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 133.86891174316406\n"
     ]
    }
   ],
   "source": [
    "print(f\"RMSE: {np.sqrt(loss)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully worked Regression example via sequential and OOP APIs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential API:  Boston house price regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, mini batch loss 4, MSE loss: 593.621\n",
      "Epoch 2, mini batch loss 4, MSE loss: 295.939\n",
      "Epoch 3, mini batch loss 4, MSE loss: 196.717\n",
      "Epoch 4, mini batch loss 4, MSE loss: 147.11\n",
      "Epoch 5, mini batch loss 4, MSE loss: 117.346\n",
      "Finished Training\n",
      " Validation  MSE loss: 613.943\n",
      " TEST  MSE loss: 522.412\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# is there a GPU availabale. If available use it\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)\n",
    "\n",
    "# load data\n",
    "boston = load_boston()\n",
    "\n",
    "X = boston['data']\n",
    "y = boston['target']\n",
    "\n",
    "in_features = X.shape[1]\n",
    "X.shape\n",
    "\n",
    "# train validation test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
    "## Scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_validation = scaler.transform(X_validation) #Transform test set with the same constants\n",
    "X_test = scaler.transform(X_test) #Transform test set with the same constants\n",
    "\n",
    "# convert numpy arrays to tensors\n",
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "X_validation_tensor = torch.from_numpy(X_validation)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "y_validation_tensor = torch.from_numpy(y_validation)\n",
    "\n",
    "# create TensorDataset in PyTorch\n",
    "boston_train = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "boston_validation = torch.utils.data.TensorDataset(X_validation_tensor, y_validation_tensor)\n",
    "boston_test = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "# create dataloader\n",
    "batch_size = 96\n",
    "trainloader_boston = torch.utils.data.DataLoader(boston_train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "validloader_boston = torch.utils.data.DataLoader(boston_validation, batch_size=X_test.shape[0], shuffle=False, num_workers=2)\n",
    "testloader_boston = torch.utils.data.DataLoader(boston_test, batch_size=X_test.shape[0], shuffle=False, num_workers=2)\n",
    "\n",
    "D_in = X_test.shape[1]\n",
    "print(D_in)\n",
    "D_hidden =20\n",
    "D_out = 1\n",
    "# Use the nn package to define our model and loss function.\n",
    "# use the sequential API makes things simple\n",
    "model = torch.nn.Sequential(    \n",
    "    torch.nn.Linear(D_in, D_hidden),   # X.matmul(W1)\n",
    "    nn.ReLU(), #nn.Sigmoid()           # Relu( X.matmul(W1))\n",
    "    nn.Linear(in_features=D_hidden, out_features=D_out)       # Relu( X.matmul(W1)).matmul(W2)\n",
    "    \n",
    ")\n",
    "# MSE loss scaffolding layer\n",
    "loss_fn = torch.nn.MSELoss(size_average=True)\n",
    "#loss_fn = nn.CrossEntropyLoss()  #for classfication \n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "epochs = range(5)\n",
    "count = 0 \n",
    "running_loss = 0.0\n",
    "for epoch in epochs:\n",
    "    running_loss = 0.0\n",
    "    for batch, data in enumerate(trainloader_boston):\n",
    "        inputs, target = data[0].to(device), data[1].to(device)\n",
    "\n",
    "\n",
    "        # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # do forward pass\n",
    "        output = model(inputs.float())\n",
    "\n",
    "        # compute loss and gradients\n",
    "        loss = loss_fn(output, torch.unsqueeze(target.float(), dim=1))\n",
    "        # get gradients w.r.t to parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # perform gradient update\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()*inputs.size(0)\n",
    "        count += inputs.size(0)\n",
    "    print(f\"Epoch {epoch+1}, mini batch loss {batch+1}, MSE loss: {np.round(running_loss/count, 3)}\")\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "count = 0 \n",
    "running_loss = 0.0\n",
    "for batch, data in enumerate(validloader_boston):\n",
    "    inputs, target = data[0].to(device), data[1].to(device)\n",
    "    # do forward pass\n",
    "    output = model(inputs.float())\n",
    "\n",
    "    # compute loss and gradients\n",
    "    loss = loss_fn(output, torch.unsqueeze(target.float(), dim=1))\n",
    "    # print statistics\n",
    "    # print statistics\n",
    "    running_loss += loss.item()*inputs.size(0)\n",
    "    count += inputs.size(0) \n",
    "    test_size +=batch_size\n",
    "print(f\" Validation  MSE loss: {np.round(running_loss/count, 3)}\")\n",
    "\n",
    "count = 0 \n",
    "running_loss = 0.0\n",
    "for batch, data in enumerate(testloader_boston):\n",
    "    inputs, target = data[0].to(device), data[1].to(device)\n",
    "    # do forward pass\n",
    "    output = model(inputs.float())\n",
    "\n",
    "    # compute loss and gradients\n",
    "    loss = loss_fn(output, torch.unsqueeze(target.float(), dim=1))\n",
    "    # print statistics\n",
    "    running_loss += loss.item()*inputs.size(0)\n",
    "    count += inputs.size(0) \n",
    "    test_size +=batch_size\n",
    "print(f\" TEST  MSE loss: {np.round(running_loss/count, 3)}\")\n",
    "\n",
    "# predict test\n",
    "output = model(X_test_tensor.float())\n",
    "# calculate loss via torch\n",
    "loss = loss_fn(output, torch.unsqueeze(y_test_tensor.float(), dim=1)).detach().numpy()/test_size\n",
    "#print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boston house price regression via OOP API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "We are working on a cpu device\n",
      "--------------------------------------------------\n",
      "NETWORK\n",
      "BaseClassifier(\n",
      "  (fc1): Linear(in_features=13, out_features=20, bias=True)\n",
      "  (fc2): Linear(in_features=20, out_features=229, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                [-1, 1, 20]             280\n",
      "            Linear-2                 [-1, 1, 1]              21\n",
      "================================================================\n",
      "Total params: 301\n",
      "Trainable params: 301\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.00\n",
      "Estimated Total Size (MB): 0.00\n",
      "----------------------------------------------------------------\n",
      "Epoch 1\n",
      "Epoch 1, batch 1, batch loss: 640.315735\n",
      "Validation 1 set: Average loss: 230.753540\n",
      "Epoch 2\n",
      "Epoch 2, batch 1, batch loss: 186.53949\n",
      "Validation 2 set: Average loss: 95.565514\n",
      "Epoch 3\n",
      "Epoch 3, batch 1, batch loss: 89.934151\n",
      "Validation 3 set: Average loss: 90.648216\n",
      "Epoch 4\n",
      "Epoch 4, batch 1, batch loss: 78.416443\n",
      "Validation 4 set: Average loss: 88.065018\n",
      "Epoch 5\n",
      "Epoch 5, batch 1, batch loss: 80.621422\n",
      "Validation 5 set: Average loss: 91.500351\n",
      "Epoch 6\n",
      "Epoch 6, batch 1, batch loss: 102.493217\n",
      "Validation 6 set: Average loss: 86.571098\n",
      "Epoch 7\n",
      "Epoch 7, batch 1, batch loss: 85.449112\n",
      "Validation 7 set: Average loss: 87.242363\n",
      "Epoch 8\n",
      "Epoch 8, batch 1, batch loss: 98.376213\n",
      "Validation 8 set: Average loss: 87.909790\n",
      "Epoch 9\n",
      "Epoch 9, batch 1, batch loss: 77.036285\n",
      "Validation 9 set: Average loss: 87.367844\n",
      "Epoch 10\n",
      "Epoch 10, batch 1, batch loss: 68.500999\n",
      "Validation 10 set: Average loss: 88.155647\n",
      "Epoch 11\n",
      "Epoch 11, batch 1, batch loss: 118.522934\n",
      "Validation 11 set: Average loss: 86.155258\n",
      "Epoch 12\n",
      "Epoch 12, batch 1, batch loss: 68.834763\n",
      "Validation 12 set: Average loss: 86.992348\n",
      "Epoch 13\n",
      "Epoch 13, batch 1, batch loss: 71.824303\n",
      "Validation 13 set: Average loss: 85.840584\n",
      "Epoch 14\n",
      "Epoch 14, batch 1, batch loss: 72.959473\n",
      "Validation 14 set: Average loss: 90.199974\n",
      "Epoch 15\n",
      "Epoch 15, batch 1, batch loss: 81.733955\n",
      "Validation 15 set: Average loss: 85.612968\n",
      "Epoch 16\n",
      "Epoch 16, batch 1, batch loss: 87.393341\n",
      "Validation 16 set: Average loss: 88.394783\n",
      "Epoch 17\n",
      "Epoch 17, batch 1, batch loss: 79.176376\n",
      "Validation 17 set: Average loss: 86.004875\n",
      "Epoch 18\n",
      "Epoch 18, batch 1, batch loss: 92.437309\n",
      "Validation 18 set: Average loss: 86.071747\n",
      "Epoch 19\n",
      "Epoch 19, batch 1, batch loss: 85.41494\n",
      "Validation 19 set: Average loss: 85.813416\n",
      "Epoch 20\n",
      "Epoch 20, batch 1, batch loss: 95.838028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/usr/local/lib/python3.7/site-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([96])) that is different to the input size (torch.Size([96, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.7/site-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([77])) that is different to the input size (torch.Size([77, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/usr/local/lib/python3.7/site-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([65])) that is different to the input size (torch.Size([65, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation 20 set: Average loss: 86.830948\n",
      "Epoch 21\n",
      "Epoch 21, batch 1, batch loss: 74.636032\n",
      "Validation 21 set: Average loss: 85.646645\n",
      "Epoch 22\n",
      "Epoch 22, batch 1, batch loss: 65.650925\n",
      "Validation 22 set: Average loss: 89.724266\n",
      "Epoch 23\n",
      "Epoch 23, batch 1, batch loss: 85.744606\n",
      "Validation 23 set: Average loss: 86.565659\n",
      "Epoch 24\n",
      "Epoch 24, batch 1, batch loss: 100.013657\n",
      "Validation 24 set: Average loss: 85.495682\n",
      "Epoch 25\n",
      "Epoch 25, batch 1, batch loss: 76.435242\n",
      "Validation 25 set: Average loss: 87.441956\n",
      "Epoch 26\n",
      "Epoch 26, batch 1, batch loss: 76.548378\n",
      "Validation 26 set: Average loss: 86.165077\n",
      "Epoch 27\n",
      "Epoch 27, batch 1, batch loss: 88.391182\n",
      "Validation 27 set: Average loss: 86.021500\n",
      "Epoch 28\n",
      "Epoch 28, batch 1, batch loss: 115.906372\n",
      "Validation 28 set: Average loss: 89.364479\n",
      "Epoch 29\n",
      "Epoch 29, batch 1, batch loss: 68.953781\n",
      "Validation 29 set: Average loss: 85.594193\n",
      "Epoch 30\n",
      "Epoch 30, batch 1, batch loss: 117.40332\n",
      "Validation 30 set: Average loss: 87.757561\n",
      "Epoch 31\n",
      "Epoch 31, batch 1, batch loss: 91.994804\n",
      "Validation 31 set: Average loss: 88.719849\n",
      "Epoch 32\n",
      "Epoch 32, batch 1, batch loss: 94.501534\n",
      "Validation 32 set: Average loss: 86.676285\n",
      "Epoch 33\n",
      "Epoch 33, batch 1, batch loss: 90.001869\n",
      "Validation 33 set: Average loss: 85.458344\n",
      "Epoch 34\n",
      "Epoch 34, batch 1, batch loss: 59.397293\n",
      "Validation 34 set: Average loss: 85.524223\n",
      "Epoch 35\n",
      "Epoch 35, batch 1, batch loss: 75.485008\n",
      "Validation 35 set: Average loss: 86.264465\n",
      "Epoch 36\n",
      "Epoch 36, batch 1, batch loss: 83.009392\n",
      "Validation 36 set: Average loss: 86.279114\n",
      "Epoch 37\n",
      "Epoch 37, batch 1, batch loss: 97.229034\n",
      "Validation 37 set: Average loss: 87.950546\n",
      "Epoch 38\n",
      "Epoch 38, batch 1, batch loss: 93.788116\n",
      "Validation 38 set: Average loss: 85.556961\n",
      "Epoch 39\n",
      "Epoch 39, batch 1, batch loss: 95.738281\n",
      "Validation 39 set: Average loss: 86.286919\n",
      "Epoch 40\n",
      "Epoch 40, batch 1, batch loss: 88.78537\n",
      "Validation 40 set: Average loss: 85.430893\n",
      "--------------------------------------------------\n",
      "Test 40 set: Average loss: 68.471840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/torch/nn/modules/loss.py:446: UserWarning: Using a target size (torch.Size([76])) that is different to the input size (torch.Size([76, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68.47183990478516"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary  #install it if necessary using !pip install torchsummary \n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "    \n",
    "# load data\n",
    "boston = load_boston()\n",
    "\n",
    "X = boston['data']\n",
    "y = boston['target']\n",
    "\n",
    "in_features = X.shape[1]\n",
    "\n",
    "# train validation test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.15, random_state=42)\n",
    "\n",
    "## Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train =      scaler.fit_transform(X_train).astype(float)\n",
    "X_validation = scaler.transform(X_validation).astype(float) #Transform valid set with the same constants\n",
    "X_test =       scaler.transform(X_test).astype(float)       #Transform test  set with the same constants\n",
    "\n",
    "# convert numpy arrays to tensors\n",
    "X_train_tensor = torch.from_numpy(X_train).float()\n",
    "X_validation_tensor = torch.from_numpy(X_validation).float()\n",
    "X_test_tensor = torch.from_numpy(X_test).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "y_test_tensor = torch.from_numpy(y_test).float()\n",
    "y_validation_tensor = torch.from_numpy(y_validation).float()\n",
    "\n",
    "# create TensorDataset in PyTorch\n",
    "train_ds = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "validation_ds = torch.utils.data.TensorDataset(X_validation_tensor, y_validation_tensor)\n",
    "test_ds = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "# create dataloader\n",
    "batch_size = 96\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valid_loader = torch.utils.data.DataLoader(validation_ds, batch_size=X_test.shape[0], shuffle=False, num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=X_test.shape[0], shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "D_in = X_test.shape[1]\n",
    "print(D_in)\n",
    "D_hidden =20\n",
    "D_out = 1\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "# use the sequential API to create a neural network regressor\n",
    "model = torch.nn.Sequential(    \n",
    "    torch.nn.Linear(D_in, D_hidden),   # X.matmul(W1)\n",
    "    nn.ReLU(), #nn.Sigmoid()           # Relu( X.matmul(W1))\n",
    "    nn.Linear(in_features=D_hidden, out_features=D_out)       # Relu( X.matmul(W1)).matmul(W2)\n",
    "    \n",
    ")\n",
    "\n",
    "# Use the OOP API to define a deep neural network model\n",
    "#\n",
    "class BaseModel(nn.Module):\n",
    "    \"\"\"Custom module for a simple  regressor\"\"\"\n",
    "    def __init__(self, in_features, size_hidden=10, n_output=1):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(in_features, size_hidden)   # hidden layer\n",
    "        self.fc2 = torch.nn.Linear(size_hidden, n_output)      # output layer\n",
    " \n",
    "    def forward(self, x):\n",
    "   \n",
    "        x = F.relu(self.fc1(x))   # activation function for hidden layer\n",
    "        #x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "        \n",
    " \n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(f\"We are working on a {device} device\")\n",
    "\n",
    "# create classifier and optimizer objects\n",
    "model = BaseModel(in_features=D_in, size_hidden = D_hidden, n_output = D_out)\n",
    "model.to(device) # put on GPU before setting up the optimizer\n",
    "print(f\"{'-'*50}\\nNETWORK\\n{clf}\")\n",
    "\n",
    "summary(model, (1, 13))\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(size_average=True)\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "loss_history = []\n",
    "acc_history = []\n",
    "\n",
    "def train_epoch(epoch, model, loss_fn, opt, train_loader):\n",
    "    clf.train() # set model in training mode (need this because of dropout)\n",
    "    running_loss = 0.0\n",
    "    count = 0\n",
    "    # dataset API gives us pythonic batching \n",
    "    for batch_id, data in enumerate(train_loader):\n",
    "        inputs, target = data[0].to(device), data[1].to(device)        \n",
    "        # 1:zero the grad, 2:forward pass, 3:calculate loss,  and 4:backprop!\n",
    "        opt.zero_grad()\n",
    "        preds = model(inputs) #prediction over the input data\n",
    "        #loss = loss_fn(preds, target)  #mean loss for this batch\n",
    "        # compute loss and gradients\n",
    "        loss = loss_fn(preds, target)\n",
    "\n",
    "        loss.backward() #calculate nabla_w\n",
    "        loss_history.append(loss.item())\n",
    "        opt.step()  #update W\n",
    "        #from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit\n",
    "        \n",
    "        running_loss += loss.item()*inputs.size(0)\n",
    "        count += inputs.size(0)\n",
    "        # print statistics\n",
    "        if batch_id % 100 == 0:    # print every 100 mini-batches\n",
    "          print(f\"Epoch {epoch+1}, batch {batch_id+1}, batch loss: {np.round(running_loss/count,6)}\")\n",
    "          running_loss = 0.0\n",
    "    return clf\n",
    "\n",
    "\n",
    "\n",
    "#from IPython.core.debugger import Pdb as pdb;    pdb().set_trace() #breakpoint; dont forget to quit\n",
    "def evaluate_model(epoch, model, loss_fn, opt, data_loader, tag = \"Test\"):\n",
    "    clf.eval() # set model in inference mode (need this because of dropout)\n",
    "    count = 0\n",
    "    overall_loss = 0.0\n",
    "\n",
    "    for i,data in enumerate(data_loader):\n",
    "        inputs, targets = data[0].to(device), data[1].to(device)                \n",
    "        outputs = model(inputs)\n",
    "        # torch.max() Returns a namedtuple (values, indices) where values is the maximum value of each row of the \n",
    "        # input tensor in the given dimension dim. And indices is the index location of each maximum value found (argmax).\n",
    "        _, predicted_classes = torch.max(outputs.data, 1)  # get the index of the max log-probabilit        \n",
    "        \n",
    "        loss = loss_fn(outputs, targets)           # compute loss value\n",
    "        loss_this_iter = loss.cpu().detach().numpy() # send loss value to CPU to save to logs\n",
    "        overall_loss += (loss.item() * inputs.size(0))  # compute total loss to save to logs\n",
    "        count += inputs.size(0)\n",
    "\n",
    "        # compute mean loss\n",
    "    overall_loss /= float(count)\n",
    "\n",
    "    print(f\"{tag} {epoch+1} set: Average loss: {overall_loss:.6f}\")\n",
    "    return overall_loss\n",
    "\n",
    "for epoch in range(40):\n",
    "    print(f\"Epoch {epoch+1}\")\n",
    "    clf = train_epoch(epoch, model, loss_fn, opt, train_loader)\n",
    "    evaluate_model(epoch,    model, loss_fn, opt, valid_loader, tag = \"Validation\")\n",
    "print(\"-\"*50)\n",
    "evaluate_model(epoch, model, loss_fn, opt, test_loader, tag=\"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Phase_0_fnicasio.ipynb",
   "provenance": [
    {
     "file_id": "1hXZ7IwHt5Kh_jIWLan4SKh0Lrkh_jIbg",
     "timestamp": 1600096563205
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
